---
title: The Galton board
date: 2023-11-02
date-format: iso
draft: false
---

```{r}
#| include: false
library(tibble)
library(ggplot2)
library(ricethemes)
library(purrr)
library(dplyr)
library(tidyr)

draw_binary <- function(n) {
  mean(sample(c(0, 1), n, replace = TRUE), na.rm = TRUE)
}


draw_samples <- function(samples, sample_size) {
  sample <- NULL
  for (i in seq(1, samples)) {
    sample[i] <- draw_binary(sample_size)
  }
  sample_frame <-
    tibble(
      id     = seq(1, samples),
      mean = sample
    )
  return(sample_frame)
}


plot_samples <- function(sample_frame, bins = 30) {
  sample_frame |>
    ggplot() + 
    aes(mean) +
    geom_histogram(
      aes(y = after_stat(density)),
      bins = {{ bins }},
      color = ctp_mocha[["peach"]],
      fill = ctp_mocha[["yellow"]],
      na.rm = TRUE
    ) +
    geom_density(color = ctp_mocha[["red"]]) +
    theme_ctp(ctp_mocha) +
    labs(x = "", y = "") +
    scale_x_continuous(limits = c(0, 1)) +
    scale_y_continuous(limits = c(0, 30)) +
    theme(
      axis.text.x = element_blank(),
      axis.text.y = element_blank()
    )
}
```

[The Galton board](https://en.wikipedia.org/wiki/Galton_board) was built in the
19th century to demonstrate the central limit theorem. Specifically, it showed
that given sufficiently large sample sizes, a collection of sample means from
the binomial distribution would approximate the normal distribution. The Galton
board is not only very satisfying to watch, it also demonstrates one of the
most important concepts for statistical inference.


![The Galton board in action](https://upload.wikimedia.org/wikipedia/commons/transcoded/d/dc/Galton_box.webm/Galton_box.webm.720p.vp9.webm){width=100%}

Assume that you want to investigate the probability of getting a tail when
flipping a coin. If you were to only flip the coin once, you would either get a
head or tail and would conclude that it is either 100% or 0% probability that
you get a tail. However, as you increase your number of throws you will
eventually conclude that the probability of getting a tail is 50%.

If you instead imagine that you and a couple of friends independently toss a
coin a small number of times each, it is unlikely that any of you will get
a tail *exactly* 50% of the time. However, the distribution of your and your
friends probabilities will be normally distributed, meaning that the
mean of the sample means will approximately be the true mean.

If the sample sizes are large, the variance of the distribution is smaller,
meaning that more samples are likely close to 50% probability.

```{r}
#| panel: tabset
#| output: asis
#| echo: false
#| warning: false
#| fig-align: center
#| out-width: "100%"

set.seed(8008)

cat("# Sample size of 10\n")
draw_samples(1000, 10) |> plot_samples()
cat("\n\n")
cat("# Sample size of 30\n")
draw_samples(1000, 30) |> plot_samples()
cat("\n\n")
cat("# Sample size of 100\n")
draw_samples(1000, 100) |> plot_samples(bins = 40)
cat("\n\n")
cat("# Sample size of 500\n")
draw_samples(1000, 500) |> plot_samples(bins = 50)
cat("\n\n")
```

While one conclusion from this is that a larger sample size provide more
accurate estimates, it also means that we can test the probability of obtaining
a sample mean by *assuming* a distribution.

In this example, we can assume that the probability of getting a tail is 50%. 
If we have only a collection of small samples, we would not be able to reject
that a small deviation from the mean is random. Meanwhile, we will be able to 
reject that a small deviation is random if we have a large sample.

One way to think about this is in terms of differences in probability
distributions. We imagine that in theory all samples should be distributed with
a mean of 50%. We then use the distribution of the sample throws to test the
probability that our sample mean is a part of a normal distribution with a mean
of 50%.

```{r}
#| echo: false
#| warning: false
#| out-width: "100%"
#| fig-cap: "What is the probability that the blue distribution, which we obtained from our sample, is the true distribution if theory suggests that the yellow distribution is the true one?"
draw_samples(1000, 30) |>
  mutate(mean = mean - 0.3) |>
  bind_cols(draw_samples(1000, 30)) |>
  rename_with(~c("id", "mean1", "id2", "mean2")) |>
  select(-id2) |>
  pivot_longer(-id) |>
  ggplot() +
  aes(value, color = name) +
  geom_density(bw = 0.03) +
  geom_vline(
    xintercept = c(0.2, 0.5),
    color = c(ctp_mocha[["blue"]], ctp_mocha[["yellow"]]),
    linetype = "dashed"
  ) +
  theme_ctp(ctp_mocha) +
  scale_color_ctp(ctp_mocha) +
  theme(
    legend.position = 0,
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
    ) +
  labs(x = "", y = "")
```

The Galton board illustrates one of the most fundamental and important ideas
in statistics in a beautiful way.
