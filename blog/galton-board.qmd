---
title: The Galton board
date: 2023-07-10
date-format: iso
draft: true
---

```{r}
#| include: false
library(tibble)
library(ggplot2)
library(ricethemes)
library(purrr)
library(dplyr)
library(tidyr)

draw_binary <- function(n) {
  mean(sample(c(0, 1), n, replace = TRUE), na.rm = TRUE)
}


draw_samples <- function(samples, sample_size) {
  sample <- NULL
  for (i in seq(1, samples)) {
    sample[i] <- draw_binary(sample_size)
  }
  sample_frame <-
    tibble(
      id     = seq(1, samples),
      mean = sample
    )
  return(sample_frame)
}


plot_samples <- function(sample_frame, bins = 30) {
  sample_frame |>
    ggplot() + 
    aes(mean) +
    geom_histogram(
      aes(y = after_stat(density)),
      bins = {{ bins }},
      color = ctp_mocha[["peach"]],
      fill = ctp_mocha[["yellow"]],
      na.rm = TRUE
    ) +
    geom_density(color = ctp_mocha[["red"]]) +
    theme_ctp(ctp_mocha) +
    labs(x = "", y = "") +
    scale_x_continuous(limits = c(0, 1)) +
    scale_y_continuous(limits = c(0, 30)) +
    theme(
      axis.text.x = element_blank(),
      axis.text.y = element_blank()
    )
}
```

[The Galton board](https://en.wikipedia.org/wiki/Galton_board) was built in the
19th century to demonstrate the central limit theorem. Specifically, it showed
that given sufficiently large sample sizes, a collection of sample means from
the binomial distribution would approximate the normal distribution. The Galton
board is not only very satisfying to watch, it also demonstrates one of the
most important concepts for statistical inference.


![The Galton board in action](https://upload.wikimedia.org/wikipedia/commons/transcoded/d/dc/Galton_box.webm/Galton_box.webm.720p.vp9.webm){width=100%}

Assume that you want to investigate the probability of getting a tail when 
flipping a coin. If you were to only flip the coin once, you would only get a 
value of either 0% or 100%. However, as you increase your number of throws you 
will eventually approach 50% which is the true value.

If you then draw many samples of a sufficiently large sample size, the sample
means will follow a normal distribution with the true mean as the mean of the
distribution. The larger the sample size is, the lower variance the
distribution will have, see the figure below.


```{r}
#| panel: tabset
#| output: asis
#| echo: false
#| warning: false
#| fig-align: center
#| out-width: "100%"

set.seed(8008)

cat("# Sample size of 10\n")
draw_samples(1000, 10) |> plot_samples()
cat("\n\n")
cat("# Sample size of 30\n")
draw_samples(1000, 30) |> plot_samples()
cat("\n\n")
cat("# Sample size of 100\n")
draw_samples(1000, 100) |> plot_samples(bins = 40)
cat("\n\n")
cat("# Sample size of 500\n")
draw_samples(1000, 500) |> plot_samples(bins = 50)
cat("\n\n")
```

While one conclusion from this is that a larger sample size provide more accurate estimates,
it also means that we can test the probability of obtaining a sample mean by
*assuming* a distribution. For example, we could then ask ourselves "what is the
probability that we obtain a sample mean of 0.2, if we believe the true mean
to be 0.5?". In the figure below, we illustrate this scenario.

```{r}
#| echo: false
#| warning: false
#| out-width: "100%"
#| fig-cap: "What is the probability that the blue distribution which we obtained from our sample is the true distirbution if theory suggests that the yellow distiribution is the true one?"
draw_samples(1000, 30) |>
  mutate(mean = mean - 0.3) |>
  bind_cols(draw_samples(1000, 30)) |>
  rename_with(~c("id", "mean1", "id2", "mean2")) |>
  select(-id2) |>
  pivot_longer(-id) |>
  ggplot() +
  aes(value, color = name) +
  geom_density(bw = 0.03) +
  geom_vline(
    xintercept = c(0.2, 0.5),
    color = c(ctp_mocha[["blue"]], ctp_mocha[["yellow"]]),
    linetype = "dashed"
  ) +
  theme_ctp(ctp_mocha) +
  scale_color_ctp(ctp_mocha) +
  theme(
    legend.position = 0,
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
    ) +
  labs(x = "", y = "")
```

This is what enables any statistical inference: we present theory supporting
a hypothesis (The population mean is 0.2), we then collect data to test the 
hypothesis which shows that the sample mean is 0.5. The probability of getting
a value of 0.5, when our theory suggests that the true mean is 0.2 is what 
determines the probability that our theory is correct, given the sample. 
